{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de805dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#过滤版本差异产生的warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9184334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(path='training_label.txt'):\n",
    "    #定义读取training所需的数据\n",
    "    #如果是'training_label.txt'，就读取label如果是'training_nolabel.txt'不需要读取label\n",
    "    if 'training_label' in path:\n",
    "        with open(path,'r',encoding = 'UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "        #每行按空格分割后，第二个符号之后都是句子的单词\n",
    "        x = [line[2:] for line in lines]\n",
    "        # 每行按空格分割后，第0个字符是label\n",
    "        y = [line[0] for line in lines]\n",
    "        return x,y\n",
    "    else:\n",
    "        with open(path,'r',encoding = 'UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x#因为这里的数据没有label，返回x就可以\n",
    "    \n",
    "    \n",
    "def load_testing_data(path='testing_data'):\n",
    "    with open(path,'r',encoding='UTF-8')as f:\n",
    "        lines = f.readlines()\n",
    "        # 第0行是表头，第一行开始是数据\n",
    "        #第0列是id，第一列是文本，按逗号分割，需要逗号之后的文本\n",
    "        X = [''.join(line.strip('\\n').split(',')[1:]).strip() for line in lines[1:]]\n",
    "        X = [sen.split(' ') for sen in X ]\n",
    "    return X\n",
    "    \n",
    "    \n",
    "def evalution(outputs,labels):\n",
    "    #outputs 预测值概率(float)\n",
    "    #labels 真实值标签（0，1）\n",
    "    outputs[outputs>=0.5] = 1#大于等于0.5为正面\n",
    "    outputs[outputs<=0.5] = 0\n",
    "    accuracy = torch.sum(torch.eq(outputs,labels)).item()\n",
    "    return accuracy\n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2149ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word 2 vector training与testing的每个单词都分别变成词向量\n",
    "\n",
    "\n",
    "#class gensim.models.word2vec.Word2Vec(\n",
    "    #sentences=None,\n",
    "    #size=100,# 词向量的维度\n",
    "    #alpha=0.025,#模型初始的学习率\n",
    "    #window=5,#在一个句子中，当前词于预测词在一个句子中的最大距离\n",
    "    #min_count=5,# 用于过滤操作，词频小于该次数的单词会被丢弃\n",
    "    #max_vocab_size=None,#设置词向量构建期间的RAM限制\n",
    "    #smaple=0.001,#高频词汇的随机降采样的培育阈值\n",
    "    #seed=1,#用于随即数发生器\n",
    "    #workers=3,#控制训练的并行数量\n",
    "    #min_alpha=0.0001,#随着训练进行，alpha线性下降到min\n",
    "    #sg=0,#用来设置训练算法，sg=0，CBOW算法，1 skip-gram算法\n",
    "    #hs=0,#设置为1 会采用hierarchica softmax 如果为0会使用negative sampling\n",
    "    #negative=5,#noise words的数量\n",
    "    #cbox_mean=1,#在CBOW算法中，这个值为0，采用上下文词向量的总和，设置为1就采用均值\n",
    "    #hashfxn=<built-in function hash>,\n",
    "    #iters=5,#算法迭代次数 \n",
    "    #null_word=0,\n",
    "    #trim_rule=None,\n",
    "    #sorted_vocab=1,#如果这个值为1，则在分配word index会对单词基于频率降序排列\n",
    "    #batch_words=10000,#每次批处理给线程传递的单词数量\n",
    "    #compute_loss=False\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e5bb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading testing data...\n",
      "Saving model\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def train_word2vec(x):\n",
    "    # 训练word to vector的word embedding\n",
    "    #window：滑动窗口的大小，min_count\n",
    "    model = Word2Vec(x, vector_size=250, window=5, min_count=5, workers=12, epochs=10, sg=1)\n",
    "    return model\n",
    "\n",
    "# 读取training数据\n",
    "print('Loading training data...')\n",
    "train_x,y = load_training_data('training_label.txt')\n",
    "train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "\n",
    "#读取testing数据\n",
    "print(\"Loading testing data...\")\n",
    "test_x = load_testing_data('testing_data.txt')\n",
    "\n",
    "# 把training中的word变成vector\n",
    "model = train_word2vec(train_x + train_x_no_label + test_x)#w2v\n",
    "\n",
    "#保存vector\n",
    "print('Saving model')\n",
    "model.save('w2v.model')\n",
    "model.save('w2v_all.model')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(outputs,threshold=0.9):\n",
    "    id = (outputs>=threshold)|(outputs<1-threshold)\n",
    "    outputs[outputs>=threshold] = 1\n",
    "    outputs[outputs<1-threshold] = 0\n",
    "    return outputs.long(),id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4252f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "# 定义一个预处理的类\n",
    "class Preprocess():\n",
    "    def __init__(self,sentences,sen_len,w2v_path):\n",
    "        self.w2v_path = w2v_path#word2vec的存储路径\n",
    "        self.sentences = sentences#句子\n",
    "        self.sen_len = sen_len#句子的固定长度\n",
    "        self.idx2word = []# 列表\n",
    "        self.word2idx = {}# 存储单词在idx2word的下标\n",
    "        self.embedding_matrix = []#存储词嵌入的向量的列表\n",
    "    \n",
    "    def get_w2v_model(self):\n",
    "        #读取之前训练好的word2vec\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "        \n",
    "    def add_embedding(self,word):\n",
    "        #这里的word只会是‘<PAD>''<UNK>'\n",
    "        #把一个随机生成的表征向量vector作为上面的嵌入\n",
    "        vector = torch.empty(1,self.embedding_dim)#创建一个未被初始化数值的tensor,tensor的大小是由size确定 \n",
    "        torch.nn.init.uniform_(vector)#从均匀分布U(a, b)中生成值，填充输入的张量或变量\n",
    "        #它的index是word2index这个词典的长度，即最后一个\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix,vector],0)#拼接向量，按行数拼接\n",
    "        \n",
    "    def make_embedding(self,load=True):\n",
    "        print('Get embedding...')\n",
    "        #获取训练好的Word2vec word embedding\n",
    "        if load:\n",
    "            print('Loading word to vec model...')\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementError\n",
    "        #遍历嵌入后的单词\n",
    "        for i,word in enumerate(self.embedding.wv.key_to_index):\n",
    "            print('get words #{}'.format(i+1),end='\\r')\n",
    "            # 新加入的word的index是word2idx这个词典的长度，即最后一个\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding.wv[word])\n",
    "        print('')\n",
    "        #把embedding_matrix变形为tensor\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        #将<PAD> <UNK>加入embedding\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words:{}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    def pad_sequence(self,sentence):\n",
    "        #将每个句子变成一样的长度,sen_len length\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            #如果句子长度小于sen_len的长度，就补PAD，缺多少单词补多少PAD\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    \n",
    "    def sentence_word2idx(self):\n",
    "        #把句子里面的字变成相应的index\n",
    "        sentence_list = []\n",
    "        for i,sen in enumerate(self.sentences):\n",
    "            print('sentence count #{}'.format(i+1),end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if(word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            #将每个句子变成一样的长度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    \n",
    "    def labels_to_tensor(self,y):\n",
    "        #把labels转成tensor\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e872d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    #Expected Data shape like :(data_num,data_len)\n",
    "    #Data can be a list of numpy array or a list of lists\n",
    "    #input data shape:(data_num,seq_len,feature_dim)\n",
    "    def __init__(self,X,y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self,idx):\n",
    "        if self.label is None:return self.data[idx]\n",
    "        return self.data[idx],self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c46878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM model\n",
    "from torch import nn\n",
    "\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self,embedding,embedding_dim,hidden_dim,num_layers,dropout=0.5,fix_embedding=True):\n",
    "        #其中参数input_size 输入数据的特征维数，通常就是embedding_dim(词向量的维度)hidden_size　LSTM中隐层的维度num_layers　循环神经网络的层数batch_first通常输入的数据shape=(batch_size,seq_length,embedding_dim),而batch_first默认是False,此时送进LSTM之前需要将batch_size与seq_length这两个维度调换\n",
    "        super(LSTM_Net,self).__init__()\n",
    "        #embedding layer\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))# embedding.size(0)词典的大小尺寸，embedding.size(1)嵌入向量的维度\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否将embedding固定住，如果fix_embedding为False，在训练过程中embedding也会跟着被训练\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True#self.embedding.weight.requires_grad 指定是否在训练过程中对词向量的权重进行微调\n",
    "        self.embedding_dim = embedding.size(1)#输入的特征维数 词向量的维度\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,num_layers=num_layers,batch_first = True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x,_ = self.lstm(inputs,None)\n",
    "        #x的dimension（batch,seq_len,hidden-size)\n",
    "        #取用LSTM最后一层的hidden state丢到分类器中\n",
    "        x = x[:,-1,:]#最后一层\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0647fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def training(batch_size,n_epoch,lr,train,valid,model,device):\n",
    "    #输出模型总的参数数量，可训练的参数数量 \n",
    "    total = sum(p.numel() for p in model.parameters())#numel函数用来获取tensor的元素数量\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\n start training,parameters total:{},trainable:{}\\n'.format(total,trainable))\n",
    "    \n",
    "    loss = nn.BCELoss()#定义损失函数为二元交叉熵损失 binary cross entropy loss 这个损失会使输入接近1的输出接近1，输入接近0输出接近0\n",
    "    t_batch = len(train)#training数据的batch size大小\n",
    "    v_batch = len(valid)#validation数据的batch size大小\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr)#Adam优化器 \n",
    "    total_loss,total_acc,best_acc = 0,0,0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss,total_acc = 0,0\n",
    "        \n",
    "        #training\n",
    "        model.train()#model的模式设为train，这样optimizer可以更新model的参数\n",
    "        for i,(inputs,labels) in enumerate(train):\n",
    "            inputs = inputs.to(device,dtype=torch.long)# 因为device为‘cuda' inputs tranform to torch.cuda.LongTensor\n",
    "            labels = labels.to(device,dtype=torch.float)#因为device为‘cuda' inputs tranform to torch.cuda.FloatTensor\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)#模型输入inputs 输出outputs\n",
    "            outputs = outputs.squeeze()#去掉最外面的dimension，好让outputs进入loss（） squeeze()可以用来去除维度，仅在维度为1时有效\n",
    "            batch_loss = loss(outputs,labels)# 计算模型此时的trainingloss\n",
    "            batch_loss.backward()# 计算loss的gradient\n",
    "            optimizer.step()\n",
    "            #计算模型此时的training accuracy\n",
    "            accuracy = evalution(outputs,labels)\n",
    "            total_acc += (accuracy/batch_size)\n",
    "            total_loss += batch_loss.item()\n",
    "        print('Epoch|{}/{}'.format(epoch+1,n_epoch))\n",
    "        print(\"Train|loss:{:.5f}Acc:{:.3f}\".format(total_loss/t_batch,total_acc/t_batch*100))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #validtion\n",
    "        model.eval()#将model的模式设为eval，这样可以固定model的参数训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss,total_acc =0,0\n",
    "            for i,(inputs,labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device,dtype=torch.long)# 因为device为‘cuda' inputs tranform to torch.cuda.LongTensor\n",
    "                labels = labels.to(device,dtype=torch.float)#因为device为‘cuda' inputs tranform to torch.cuda.FloatTensor\n",
    "                outputs = model(inputs)#模型输入inputs 输出outputs\n",
    "                outputs = outputs.squeeze()#去掉最外面的dimension，好让outputs进入loss（） squeeze()可以用来去除维度，仅在维度为1时有效\n",
    "                batch_loss = loss(outputs,labels)# 计算模型此时的trainingloss\n",
    "                accuracy = evalution(outputs,labels)\n",
    "                total_acc += (accuracy/batch_size)\n",
    "                total_loss += batch_loss.item()\n",
    "            print(\"Valid|loss:{:.5f}Acc:{:.3f}\".format(total_loss/v_batch,total_acc/v_batch*100))\n",
    "            if total_acc>best_acc:\n",
    "                #如果validation的结果优于之前所有的结果，就把当下的模型保存下来，用于之后的testing\n",
    "                best_acc=total_acc\n",
    "                torch.save(model,'ckpt.model')\n",
    "        print('-----------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c03c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Get embedding...\n",
      "Loading word to vec model...\n",
      "get words #24694\n",
      "total words:24696\n",
      "Train|Len:180000200000\n",
      "Valid |Len:20000\n",
      "\n",
      " start training,parameters total:6415351,trainable:241351\n",
      "\n",
      "Epoch|1/5\n",
      "Train|loss:0.49745Acc:74.985\n",
      "Valid|loss:0.45371Acc:78.120\n",
      "-----------------------------------------\n",
      "Epoch|2/5\n",
      "Train|loss:0.44246Acc:79.167\n",
      "Valid|loss:0.43972Acc:78.707\n",
      "-----------------------------------------\n",
      "Epoch|3/5\n",
      "Train|loss:0.42706Acc:80.112\n",
      "Valid|loss:0.42761Acc:79.454\n",
      "-----------------------------------------\n",
      "Epoch|4/5\n",
      "Train|loss:0.41419Acc:80.871\n",
      "Valid|loss:0.42308Acc:79.837\n",
      "-----------------------------------------\n",
      "Epoch|5/5\n",
      "Train|loss:0.40315Acc:81.485\n",
      "Valid|loss:0.42105Acc:79.752\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "device = torch.device('cuda')\n",
    "#定义句子长度，要不要固定embedding，batch size，epoch，lr，w2vpath\n",
    "sen_len = 20\n",
    "fix_embedding = True#fix embedding during training\n",
    "batch_size = 128\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "w2v_path = 'w2v_all.model'\n",
    "print(\"Loading data\")#读取’training_label.txt','training_nolabel.txt'\n",
    "trainx,y = load_training_data('training_label.txt')\n",
    "train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "\n",
    "#对input与label做预处理\n",
    "preprocess = Preprocess(train_x,sen_len,w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)# 以上函数的定义都可以在Preprocess找到\n",
    "\n",
    "\n",
    "#定义模型\n",
    "model = LSTM_Net(embedding,embedding_dim=250,hidden_dim=150,num_layers=1,dropout=0.5,fix_embedding = fix_embedding)\n",
    "model = model.to(device)#device = cuda\n",
    "\n",
    "#把data分为training data validation data\n",
    "X_train,X_val,y_train,y_val = train_test_split(train_x,y,test_size =0.1,random_state=1,stratify = y)#验证集占比10%，分割方式按照y的分类方式\n",
    "print('Train|Len:{}\\nValid |Len:{}'.format(len(y_train),len(y_val)))\n",
    "\n",
    "#把data做成dataset供dataloader取用\n",
    "train_dataset = TwitterDataset(X=X_train,y=y_train)\n",
    "val_dataset = TwitterDataset(X=X_val,y=y_val)\n",
    "\n",
    "#data转换为batch of tensors\n",
    "train_loader = DataLoader(train_dataset,batch_size = batch_size,shuffle=True,num_workers = 0)\n",
    "val_loader = DataLoader(val_dataset,batch_size = batch_size,shuffle = False,num_workers = 0)\n",
    "\n",
    "training(batch_size,epoch,lr,train_loader,val_loader,model,device)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "835bdf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(batch_size,test_loader,model,device):\n",
    "    model.eval()\n",
    "    ret_output= []\n",
    "    with torch.no_grad():\n",
    "        for i,inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device,dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1#大于等于0.5为正面\n",
    "            outputs[outputs<0.5]= 0\n",
    "            ret_output += outputs.int().tolist()\n",
    "            \n",
    "    return ret_output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3799809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data ...\n",
      "Get embedding...\n",
      "Loading word to vec model...\n",
      "get words #24694\n",
      "total words:24696\n",
      "sentence count #200000\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "# 测试模型并作预测\n",
    "\n",
    "# 读取测试数据test_x\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data('testing_data.txt')\n",
    "# 对test_x作预处理\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "# 读取模型\n",
    "print('\\nload model ...')\n",
    "model = torch.load('ckpt.model')\n",
    "# 测试模型\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "# 保存为 csv \n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv('predict RNN emtion.csv', index=False)\n",
    "print(\"Finish Predicting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650e461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Augustine] *",
   "language": "python",
   "name": "conda-env-Augustine-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
