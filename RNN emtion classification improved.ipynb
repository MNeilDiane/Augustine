{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac9c85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "loading testing data ...\n",
      "saving model ...\n",
      "loading data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #55777\n",
      "total words: 55779\n",
      "Train | Len:180000 8614\n",
      "Valid | Len:20000\n",
      "\n",
      "start training, parameter total:14485377, trainable:205953\n",
      "\n",
      "torch.Size([200000, 20])\n",
      "Epoch | 1/11\n",
      "Train | Loss:0.49978 Acc: 74.823\n",
      "-----------------------------------------------\n",
      "torch.Size([200000, 20])\n",
      "Epoch | 2/11\n",
      "Train | Loss:0.44151 Acc: 79.389\n",
      "-----------------------------------------------\n",
      "torch.Size([200000, 20])\n",
      "Epoch | 3/11\n",
      "Train | Loss:0.42651 Acc: 80.319\n",
      "-----------------------------------------------\n",
      "torch.Size([200000, 20])\n",
      "Epoch | 4/11\n",
      "Train | Loss:0.41505 Acc: 80.979\n",
      "-----------------------------------------------\n",
      "torch.Size([200000, 20])\n",
      "Epoch | 5/11\n",
      "Train | Loss:0.40412 Acc: 81.572\n",
      "-----------------------------------------------\n",
      "torch.Size([656981, 20])\n",
      "Epoch | 6/11\n",
      "Train | Loss:0.13776 Acc: 94.490\n",
      "-----------------------------------------------\n",
      "torch.Size([876012, 20])\n",
      "Epoch | 7/11\n",
      "Train | Loss:0.11658 Acc: 95.967\n",
      "-----------------------------------------------\n",
      "torch.Size([1018370, 20])\n",
      "Epoch | 8/11\n",
      "Train | Loss:0.10984 Acc: 96.565\n",
      "-----------------------------------------------\n",
      "torch.Size([1087177, 20])\n",
      "Epoch | 9/11\n",
      "Train | Loss:0.10575 Acc: 96.851\n",
      "-----------------------------------------------\n",
      "torch.Size([1130786, 20])\n",
      "Epoch | 10/11\n",
      "Train | Loss:0.10186 Acc: 97.046\n",
      "-----------------------------------------------\n",
      "torch.Size([1182102, 20])\n",
      "Epoch | 11/11\n",
      "Train | Loss:0.10038 Acc: 97.145\n",
      "-----------------------------------------------\n",
      "loading testing data ...\n",
      "sentence count #200000\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "# 设置后可以过滤一些无用的warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# utils.py\n",
    "# 用来定义一些之后常用到的函数\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_training_data(path='training_label.txt'):\n",
    "    # 读取 training 需要的数据\n",
    "    # 如果是 'training_label.txt'，需要读取 label，如果是 'training_nolabel.txt'，不需要读取 label\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r',encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            # lines是二维数组，第一维是行line(按回车分割)，第二维是每行的单词(按空格分割)\n",
    "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "        # 每行按空格分割后，第2个符号之后都是句子的单词\n",
    "        x = [line[2:] for line in lines]\n",
    "        # 每行按空格分割后，第0个符号是label\n",
    "        y = [line[0] for line in lines]\n",
    "        return x, y\n",
    "    else:\n",
    "        with open(path, 'r',encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            # lines是二维数组，第一维是行line(按回车分割)，第二维是每行的单词(按空格分割)\n",
    "            x = [line.strip('\\n').split(' ') for line in lines]\n",
    "        return x\n",
    "\n",
    "def load_testing_data(path='testing_data'):\n",
    "    # 读取 testing 需要的数据\n",
    "    with open(path, 'r',encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        # 第0行是表头，从第1行开始是数据\n",
    "        # 第0列是id，第1列是文本，按逗号分割，需要逗号之后的文本\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
    "        X = [sen.split(' ') for sen in X]\n",
    "    return X\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    # outputs => 预测值，概率（float）\n",
    "    # labels => 真实值，标签（0或1）\n",
    "    outputs[outputs>=0.5] = 1 # 大于等于 0.5 为正面\n",
    "    outputs[outputs<0.5] = 0 # 小于 0.5 为负面\n",
    "    accuracy = torch.sum(torch.eq(outputs, labels)).item()\n",
    "    return accuracy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec(x):\n",
    "    # 训练 word to vector 的 word embedding\n",
    "    # window：滑动窗口的大小，min_count：过滤掉语料中出现频率小于min_count的词\n",
    "    model = Word2Vec(x, vector_size=256, window=5, min_count=5, workers=12, epochs=10, sg=1)\n",
    "    return model\n",
    "\n",
    "# 读取 training 数据\n",
    "print(\"loading training data ...\")\n",
    "train_x, y = load_training_data('training_label.txt')\n",
    "train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "\n",
    "# 读取 testing 数据\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data('testing_data.txt')\n",
    "\n",
    "# 把 training 中的 word 变成 vector\n",
    "model = train_word2vec(train_x + train_x_no_label + test_x) # w2v_all\n",
    "# model = train_word2vec(train_x + test_x) # w2v\n",
    "\n",
    "# 保存 vector\n",
    "print(\"saving model ...\")\n",
    "model.save('w2v_all.model')\n",
    "# model.save('w2v.model')\n",
    "\n",
    "# 数据预处理\n",
    "class Preprocess():\n",
    "    def __init__(self, sen_len, w2v_path):\n",
    "        self.w2v_path = w2v_path   # word2vec的存储路径\n",
    "        self.sen_len = sen_len    # 句子的固定长度\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        # 读取之前训练好的 word2vec \n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        # 这里的 word 只会是 \"<PAD>\" 或 \"<UNK>\" \n",
    "        # 把一个随机生成的表征向量 vector 作为 \"<PAD>\" 或 \"<UNK>\" 的嵌入\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        # 它的 index 是 word2idx 这个词典的长度，即最后一个\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        # 获取训练好的 Word2vec word embedding\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # 遍历嵌入后的单词\n",
    "        for i, word in enumerate(self.embedding.wv.key_to_index):\n",
    "            print('get words #{}'.format(i+1), end='\\r')\n",
    "            # 新加入的 word 的 index 是 word2idx 这个词典的长度，即最后一个\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding.wv[word])\n",
    "        print('')\n",
    "        # 把 embedding_matrix 变成 tensor \n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        # 将 <PAD> 和 <UNK> 加入 embedding \n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    def pad_sequence(self, sentence):\n",
    "        # 将每个句子变成一样的长度，即 sen_len 的长度\n",
    "        if len(sentence) > self.sen_len:\n",
    "        # 如果句子长度大于 sen_len 的长度，就截断\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "        # 如果句子长度小于 sen_len 的长度，就补上 <PAD> 符号，缺多少个单词就补多少个 <PAD> \n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "\n",
    "    def sentence_word2idx(self, sentences):\n",
    "        # 把句子里面的字变成相对应的 index\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(sentences):\n",
    "            print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                # 没有出现过的单词就用 <UNK> 表示\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # 将每个句子变成一样的长度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "\n",
    "    def labels_to_tensor(self, y):\n",
    "        # 把 labels 转成 tensor\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)\n",
    "\n",
    "    def get_pad(self):\n",
    "      return self.word2idx[\"<PAD>\"]\n",
    "\t\t  \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    Data can be a list of numpy array or a list of lists\n",
    "    input data shape : (data_num, seq_len, feature_dim)\n",
    "    \n",
    "    __len__ will return the number of data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "from torch import nn\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否将 embedding 固定住，如果 fix_embedding 为 False，在训练过程中，embedding 也会跟着被训练\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                          nn.Linear(hidden_dim, 64),\n",
    "                          nn.Dropout(dropout),\n",
    "                          nn.Linear(64, 1),\n",
    "                          nn.Sigmoid() )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最后一层的 hidden state 丢到分类器中\n",
    "        x = x[:, -1, :] \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\t\t\n",
    "def add_label(outputs, threshold=0.9):\n",
    "    id = (outputs>=threshold) | (outputs<1-threshold)\n",
    "    outputs[outputs>=threshold] = 1 # 大于等于 threshold 为正面\n",
    "    outputs[outputs<1-threshold] = 0 # 小于 threshold 为负面\n",
    "    return outputs.long(), id\n",
    "\t\n",
    "def training(batch_size, n_epoch, lr, X_train, y_train, val_loader, train_x_no_label, model, device):\n",
    "    # 输出模型总的参数数量、可训练的参数数量\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    \n",
    "    loss = nn.BCELoss() # 定义损失函数为二元交叉熵损失 binary cross entropy loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # optimizer用Adam，设置适当的学习率lr\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        print(X_train.shape)\n",
    "        train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 0) \n",
    "        total_loss, total_acc = 0, 0\n",
    "\n",
    "        # training\n",
    "        model.train() # 将 model 的模式设为 train，这样 optimizer 就可以更新 model 的参数\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long) # 因为 device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.float) # 因为 device 为 \"cuda\"，将 labels 转成 torch.cuda.FloatTensor，loss()需要float\n",
    "\t\t\t\n",
    "            optimizer.zero_grad() # 由于 loss.backward() 的 gradient 会累加，所以每一个 batch 后需要归零\n",
    "            outputs = model(inputs) # 模型输入Input，输出output\n",
    "            outputs = outputs.squeeze() # 去掉最外面的 dimension，好让 outputs 可以丢进 loss()\n",
    "            batch_loss = loss(outputs, labels) # 计算模型此时的 training loss\n",
    "            batch_loss.backward() # 计算 loss 的 gradient\n",
    "            optimizer.step() # 更新模型参数\n",
    "\t\t\t\n",
    "            accuracy = evaluation(outputs, labels) # 计算模型此时的 training accuracy\n",
    "            total_acc += (accuracy / batch_size)\n",
    "            total_loss += batch_loss.item()\n",
    "        print('Epoch | {}/{}'.format(epoch+1,n_epoch))\n",
    "        t_batch = len(train_loader) \n",
    "        print('Train | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "\n",
    "        model.eval() # 将 model 的模式设为 eval，这样 model 的参数就会被固定住\n",
    "        # self-training\n",
    "        if epoch >= 4 :\n",
    "            train_no_label_dataset = TwitterDataset(X=train_x_no_label, y=None)  \n",
    "            train_no_label_loader = DataLoader(train_no_label_dataset, batch_size = batch_size, shuffle = False, num_workers = 0) \n",
    "            train_x_no_label_tmp = torch.Tensor([[]])\n",
    "            with torch.no_grad():\n",
    "                for i, (inputs) in enumerate(train_no_label_loader):\n",
    "                    inputs = inputs.to(device, dtype=torch.long) # 因为 device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n",
    "                  \n",
    "                    outputs = model(inputs) # 模型输入Input，输出output\n",
    "                    outputs = outputs.squeeze() # 去掉最外面的 dimension，好让 outputs 可以丢进 loss()\n",
    "                    labels, id = add_label(outputs)\n",
    "                    # 加入新标注的数据\n",
    "                    X_train = torch.cat((X_train.to(device), inputs[id]), dim=0)\n",
    "                    y_train = torch.cat((y_train.to(device), labels[id]), dim=0)\n",
    "                    if i == 0: \n",
    "                      train_x_no_label = inputs[~id]\n",
    "                    else: \n",
    "                      train_x_no_label = torch.cat((train_x_no_label.to(device), inputs[~id]), dim=0)\n",
    "\n",
    "        # validation\n",
    "        if val_loader is None:\n",
    "            torch.save(model, \"ckpt.model\")\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                total_loss, total_acc = 0, 0\n",
    "            \n",
    "                for i, (inputs, labels) in enumerate(val_loader):\n",
    "                    inputs = inputs.to(device, dtype=torch.long) # 因为 device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n",
    "                    labels = labels.to(device, dtype=torch.float) # 因为 device 为 \"cuda\"，将 labels 转成 torch.cuda.FloatTensor，loss()需要float\n",
    "            \n",
    "                    outputs = model(inputs) # 模型输入Input，输出output\n",
    "                    outputs = outputs.squeeze() # 去掉最外面的 dimension，好让 outputs 可以丢进 loss()\n",
    "                    batch_loss = loss(outputs, labels) # 计算模型此时的 training loss\n",
    "                    accuracy = evaluation(outputs, labels) # 计算模型此时的 training accuracy\n",
    "                    total_acc += (accuracy / batch_size)\n",
    "                    total_loss += batch_loss.item()\n",
    "\n",
    "                v_batch = len(val_loader)\n",
    "                print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "                if total_acc > best_acc:\n",
    "                    # 如果 validation 的结果优于之前所有的結果，就把当下的模型保存下来，用于之后的testing\n",
    "                    best_acc = total_acc\n",
    "                    torch.save(model, \"ckpt.model\")\n",
    "        print('-----------------------------------------------')\n",
    "\t\t\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 通过 torch.cuda.is_available() 的值判断是否可以使用 GPU ，如果可以的话 device 就设为 \"cuda\"，没有的话就设为 \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义句子长度、要不要固定 embedding、batch 大小、要训练几个 epoch、 学习率的值、 w2v的路径\n",
    "sen_len = 20\n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 128\n",
    "epoch = 11\n",
    "lr = 8e-4\n",
    "w2v_path = 'w2v_all.model' \n",
    "\n",
    "print(\"loading data ...\") # 读取 'training_label.txt'  'training_nolabel.txt' \n",
    "train_x, y = load_training_data('training_label.txt')\n",
    "train_x_no_label = load_training_data('training_nolabel.txt')\n",
    "\n",
    "# 对 input 跟 labels 做预处理\n",
    "preprocess = Preprocess(sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "\n",
    "train_x = preprocess.sentence_word2idx(train_x)\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "train_x_no_label = preprocess.sentence_word2idx(train_x_no_label)\n",
    "\n",
    "# 把 data 分为 training data 和 validation data（将一部分 training data 作为 validation data）\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_x, y, test_size = 0.1, random_state = 1, stratify = y)\n",
    "print('Train | Len:{} \\nValid | Len:{}'.format(len(y_train), len(y_val)))\n",
    "\n",
    "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "# 定义模型\n",
    "model = LSTM_Net(embedding, embedding_dim=256, hidden_dim=128, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
    "model = model.to(device) # device为 \"cuda\"，model 使用 GPU 来训练（inputs 也需要是 cuda tensor）\n",
    "\n",
    "# 开始训练\n",
    "# training(batch_size, epoch, lr, X_train, y_train, val_loader, train_x_no_label, model, device)\n",
    "training(batch_size, epoch, lr, train_x, y, None, train_x_no_label, model, device)\n",
    "\t\n",
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()     # 将 model 的模式设为 eval，这样 model 的参数就会被固定住\n",
    "    ret_output = []   # 返回的output\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1 # 大于等于0.5为正面\n",
    "            outputs[outputs<0.5] = 0 # 小于0.5为负面\n",
    "            ret_output += outputs.int().tolist()\n",
    "    \n",
    "    return ret_output\n",
    "\t\n",
    "# 测试模型并作预测\n",
    "# 读取测试数据test_x\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data('testing_data.txt')\n",
    "# 对test_x作预处理\n",
    "test_x = preprocess.sentence_word2idx(test_x)\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "# 读取模型\n",
    "print('\\nload model ...')\n",
    "model = torch.load('ckpt.model')\n",
    "# 测试模型\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "# 保存为 csv \n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv('predict.csv', index=False)\n",
    "print(\"Finish Predicting\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Augustine] *",
   "language": "python",
   "name": "conda-env-Augustine-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
