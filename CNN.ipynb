{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e970ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b139eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个读取图片的函数readfile()\n",
    "def readfile(path,label):\n",
    "    # bool label 代表我们是否需要返回y值\n",
    "    image_dir = sorted(os.listdir(path))#sorted防止乱序\n",
    "    x = np.zeros((len(image_dir),128,128,3),dtype=np.uint8)\n",
    "    #x存储图片，每张图片都是128*128*3（三通道） \n",
    "    #y存储标签，每个标签大小为1\n",
    "    y = np.zeros((len(image_dir)),dtype=np.uint8)\n",
    "    for i,file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path,file))\n",
    "        #利用cv2.resize（）将不同大小的图片统一为128*128\n",
    "        x[i,:,:]=cv2.resize(img,(128,128))\n",
    "        if label:\n",
    "            y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "        return x,y\n",
    "    else:\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61267ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data\n",
      "Size of training set =9866\n",
      "Size of validation set =3430\n",
      "Size of testing set =3347\n"
     ]
    }
   ],
   "source": [
    "#分别将training set,validation set,testing set读入\n",
    "workspace_dir='./food-11'\n",
    "print(\"Reading Data\")\n",
    "train_x,train_y = readfile(os.path.join(workspace_dir,\"training\"), True)\n",
    "print(\"Size of training set ={}\".format(len(train_x)))\n",
    "val_x,val_y= readfile(os.path.join(workspace_dir,\"validation\"), True)\n",
    "print(\"Size of validation set ={}\".format(len(val_x)))\n",
    "test_x= readfile(os.path.join(workspace_dir,\"testing\"), False)\n",
    "print(\"Size of testing set ={}\".format(len(test_x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddf570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training 通过随机旋转水平翻转来进行 data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),#随机翻转图片\n",
    "    transforms.RandomRotation(15),#旋转图片\n",
    "    transforms.ToTensor(),# 将图片变成张量，并把数值normalize到[0,1]\n",
    "    \n",
    "])\n",
    "#testing 不需要数据增强\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self,x,y=None,transform=None):\n",
    "        self.x = x\n",
    "        #label 是LongTensor型\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self,index):\n",
    "        X =self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X,Y\n",
    "        else:\n",
    "            return X\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7cb461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_set = ImgDataset(train_x,train_y,train_transform)\n",
    "val_set = ImgDataset(val_x,val_y,test_transform)\n",
    "train_loader = DataLoader(train_set,batch_size = batch_size,shuffle = True)#set to True to have the data reshuffled at every epoch (default: False).\n",
    "val_loader = DataLoader(val_set,batch_size = batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f690b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个卷积神经网络 再是一个全连接的前向传播神经网络\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier,self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
    "        #torch.nn.Maxpool2d(kernel_size,stride,padding)\n",
    "        #input 维度[3,128,128]\n",
    "        self.cnn = nn.Sequential(\n",
    "             nn.Conv2d(3,64,3,1,1),#输出[64,128,128]\n",
    "             nn.BatchNorm2d(64),#(防止梯度消失或爆炸) x-mean(x)/(Var(x)^0.5+eps )*gamma +beta       \n",
    "             nn.ReLU(),#ReLU是将所有的负值都设为零'\n",
    "             nn.MaxPool2d(2,2,0),#输出[64,64,64] class torch.nn.MaxPool2d (kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "             \n",
    "             nn.Conv2d(64,128,3,1,1),#padding(int or tuple, optional) - 输入的每一条边补充0的层数，主要用于边缘处填充输出[128,64,64]\n",
    "             nn.BatchNorm2d(128),\n",
    "             nn.ReLU(),\n",
    "             nn.MaxPool2d(2,2,0),#输出[128,32,32]\n",
    "             \n",
    "             nn.Conv2d(128,256,3,1,1),#输出[256,32,32]\n",
    "             nn.BatchNorm2d(256),\n",
    "             nn.ReLU(),\n",
    "             nn.MaxPool2d(2,2,0),#输出[256,16,16]\n",
    "             \n",
    "             nn.Conv2d(256,512,3,1,1),#输出[512,16,16]\n",
    "             nn.BatchNorm2d(512),\n",
    "             nn.ReLU(),\n",
    "             nn.MaxPool2d(2,2,0),#输出[512,8,8]\n",
    "             \n",
    "             nn.Conv2d(512,512,3,1,1),#输出[512,8,8]\n",
    "             nn.BatchNorm2d(512),\n",
    "             nn.ReLU(),\n",
    "             nn.MaxPool2d(2,2,0),#输出[512,4,4]\n",
    "             \n",
    "             \n",
    "        \n",
    "         )#全连接的前向传播网络\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,11)#11个分类\n",
    "        \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0],-1)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41599f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 39.21 sec(s) Train Acc: 0.237077 Loss: 0.270621 | Val Acc: 0.248688 loss: 0.261966\n",
      "[002/030] 30.88 sec(s) Train Acc: 0.285627 Loss: 0.251641 | Val Acc: 0.263265 loss: 0.267512\n",
      "[003/030] 31.22 sec(s) Train Acc: 0.322522 Loss: 0.239698 | Val Acc: 0.330904 loss: 0.233009\n",
      "[004/030] 30.64 sec(s) Train Acc: 0.355869 Loss: 0.228165 | Val Acc: 0.311079 loss: 0.266576\n",
      "[005/030] 29.77 sec(s) Train Acc: 0.390939 Loss: 0.216164 | Val Acc: 0.400583 loss: 0.212695\n",
      "[006/030] 30.23 sec(s) Train Acc: 0.422765 Loss: 0.204593 | Val Acc: 0.400292 loss: 0.217368\n",
      "[007/030] 30.06 sec(s) Train Acc: 0.455909 Loss: 0.193595 | Val Acc: 0.466764 loss: 0.190233\n",
      "[008/030] 30.19 sec(s) Train Acc: 0.472836 Loss: 0.186033 | Val Acc: 0.474636 loss: 0.190676\n",
      "[009/030] 30.21 sec(s) Train Acc: 0.500608 Loss: 0.177481 | Val Acc: 0.488338 loss: 0.192815\n",
      "[010/030] 30.06 sec(s) Train Acc: 0.522502 Loss: 0.169804 | Val Acc: 0.534694 loss: 0.173239\n",
      "[011/030] 30.18 sec(s) Train Acc: 0.546422 Loss: 0.161342 | Val Acc: 0.555394 loss: 0.161708\n",
      "[012/030] 29.78 sec(s) Train Acc: 0.570241 Loss: 0.152809 | Val Acc: 0.564723 loss: 0.160304\n",
      "[013/030] 29.68 sec(s) Train Acc: 0.597304 Loss: 0.144596 | Val Acc: 0.588047 loss: 0.153853\n",
      "[014/030] 29.60 sec(s) Train Acc: 0.619299 Loss: 0.137201 | Val Acc: 0.625073 loss: 0.147192\n",
      "[015/030] 29.66 sec(s) Train Acc: 0.640989 Loss: 0.130936 | Val Acc: 0.604665 loss: 0.149087\n",
      "[016/030] 29.72 sec(s) Train Acc: 0.655889 Loss: 0.123760 | Val Acc: 0.601749 loss: 0.150032\n",
      "[017/030] 29.82 sec(s) Train Acc: 0.666937 Loss: 0.119415 | Val Acc: 0.611953 loss: 0.151528\n",
      "[018/030] 29.80 sec(s) Train Acc: 0.685181 Loss: 0.114480 | Val Acc: 0.627988 loss: 0.143010\n",
      "[019/030] 29.80 sec(s) Train Acc: 0.699473 Loss: 0.107654 | Val Acc: 0.633236 loss: 0.141501\n",
      "[020/030] 29.75 sec(s) Train Acc: 0.712548 Loss: 0.102266 | Val Acc: 0.646647 loss: 0.143105\n",
      "[021/030] 29.82 sec(s) Train Acc: 0.733732 Loss: 0.095533 | Val Acc: 0.647813 loss: 0.138927\n",
      "[022/030] 30.31 sec(s) Train Acc: 0.752281 Loss: 0.089808 | Val Acc: 0.653644 loss: 0.139943\n",
      "[023/030] 29.91 sec(s) Train Acc: 0.763430 Loss: 0.086160 | Val Acc: 0.662682 loss: 0.137751\n",
      "[024/030] 29.77 sec(s) Train Acc: 0.782688 Loss: 0.078804 | Val Acc: 0.649854 loss: 0.147377\n",
      "[025/030] 30.05 sec(s) Train Acc: 0.790391 Loss: 0.075920 | Val Acc: 0.630612 loss: 0.158116\n",
      "[026/030] 30.13 sec(s) Train Acc: 0.798703 Loss: 0.071727 | Val Acc: 0.672303 loss: 0.143314\n",
      "[027/030] 29.92 sec(s) Train Acc: 0.814210 Loss: 0.066419 | Val Acc: 0.662682 loss: 0.151532\n",
      "[028/030] 29.96 sec(s) Train Acc: 0.829921 Loss: 0.061964 | Val Acc: 0.653353 loss: 0.155121\n",
      "[029/030] 30.10 sec(s) Train Acc: 0.838030 Loss: 0.057685 | Val Acc: 0.657726 loss: 0.153875\n",
      "[030/030] 30.22 sec(s) Train Acc: 0.847355 Loss: 0.055279 | Val Acc: 0.649563 loss: 0.170112\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Classifier().cuda() #用cuda加速\n",
    "loss = nn.CrossEntropyLoss() # 因为是分类任务，所以使用交叉熵损失 \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 使用Adam优化器\n",
    "num_epoch = 30 #迭代次数\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train() # 确保 model 是在 训练 model (开启 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 将模型参数的梯度 gradient 归零\n",
    "        train_pred = model(data[0].cuda()) # 利用 model 得到预测的概率分布，这边实际上是调用模型的 forward 函数\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 计算 loss （注意 prediction 跟 label 必须同时在 CPU 或是 GPU 上）\n",
    "        batch_loss.backward() # 利用 back propagation 算出每个参数的 gradient\n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新参数\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())#numpy.argmax(array, axis) 用于返回一个numpy数组中最大值的索引值。当一组中同时出现几个最大值时，返回第一个最大值的索引值。\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    #验证集val\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #将结果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f67b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0) # 将train_x和val_x拼接起来 二维数组 axis=0，进行列的拼接\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0) # 将train_y和val_y拼接起来\n",
    "train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b5abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 36.40 sec(s) Train Acc: 0.244209 Loss: 0.266457\n",
      "[002/030] 36.51 sec(s) Train Acc: 0.292870 Loss: 0.247876\n",
      "[003/030] 37.08 sec(s) Train Acc: 0.331077 Loss: 0.234951\n",
      "[004/030] 37.54 sec(s) Train Acc: 0.364170 Loss: 0.223731\n",
      "[005/030] 41.52 sec(s) Train Acc: 0.404708 Loss: 0.209303\n",
      "[006/030] 37.66 sec(s) Train Acc: 0.449985 Loss: 0.196587\n",
      "[007/030] 37.20 sec(s) Train Acc: 0.473375 Loss: 0.185052\n",
      "[008/030] 37.23 sec(s) Train Acc: 0.505490 Loss: 0.174797\n",
      "[009/030] 37.25 sec(s) Train Acc: 0.531363 Loss: 0.165697\n",
      "[010/030] 36.99 sec(s) Train Acc: 0.557010 Loss: 0.156604\n",
      "[011/030] 37.57 sec(s) Train Acc: 0.586417 Loss: 0.148423\n",
      "[012/030] 37.03 sec(s) Train Acc: 0.608830 Loss: 0.141157\n",
      "[013/030] 36.88 sec(s) Train Acc: 0.631844 Loss: 0.132772\n",
      "[014/030] 37.80 sec(s) Train Acc: 0.649970 Loss: 0.126077\n",
      "[015/030] 37.59 sec(s) Train Acc: 0.667870 Loss: 0.118754\n",
      "[016/030] 36.56 sec(s) Train Acc: 0.692238 Loss: 0.111874\n",
      "[017/030] 36.48 sec(s) Train Acc: 0.702241 Loss: 0.106975\n",
      "[018/030] 37.32 sec(s) Train Acc: 0.721495 Loss: 0.099995\n",
      "[019/030] 36.79 sec(s) Train Acc: 0.738342 Loss: 0.094387\n",
      "[020/030] 37.06 sec(s) Train Acc: 0.759100 Loss: 0.087445\n",
      "[021/030] 37.33 sec(s) Train Acc: 0.767750 Loss: 0.083848\n",
      "[022/030] 37.60 sec(s) Train Acc: 0.781889 Loss: 0.079232\n",
      "[023/030] 37.74 sec(s) Train Acc: 0.793622 Loss: 0.074846\n",
      "[024/030] 37.83 sec(s) Train Acc: 0.811673 Loss: 0.068756\n",
      "[025/030] 37.46 sec(s) Train Acc: 0.820698 Loss: 0.065210\n",
      "[026/030] 37.21 sec(s) Train Acc: 0.833258 Loss: 0.061016\n",
      "[027/030] 37.26 sec(s) Train Acc: 0.838297 Loss: 0.058489\n",
      "[028/030] 38.31 sec(s) Train Acc: 0.857100 Loss: 0.052203\n",
      "[029/030] 38.21 sec(s) Train Acc: 0.867404 Loss: 0.049219\n",
      "[030/030] 37.15 sec(s) Train Acc: 0.866727 Loss: 0.048653\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda() # cuda加速\n",
    "loss = nn.CrossEntropyLoss() # 因为是分类任务，所以使用交叉熵损失 \n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #将结果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d325b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb9cf0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "        # 预测值中概率最大的下标即为模型预测的食物标签\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dbd5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将预测结果写入 csv \n",
    "with open(\"predict.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac8381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Augustine] *",
   "language": "python",
   "name": "conda-env-Augustine-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
